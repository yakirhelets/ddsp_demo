{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDSPDemo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+PAgEhI9TmHuGf796KPLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakirhelets/ddsp_demo/blob/master/DDSPDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "x8Ocf-w4XHw1",
        "colab": {}
      },
      "source": [
        "#@title #Install and Import\n",
        "\n",
        "#@markdown Install ddsp, define some helper functions, and clone the sources required for the demo. This transfers a lot of data and _should take a few minutes_.\n",
        "#@markdown * Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "#@markdown * Press the ▶️ button on the left of each of the cells\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "print('Installing from pip package...')\n",
        "!pip install -qU ddsp\n",
        "print('\\nHang on! Copying sources...')\n",
        "!git clone https://github.com/yakirhelets/ddsp_demo.git\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab.colab_utils import (download, play, record, specplot, upload,\n",
        "                                    DEFAULT_SAMPLE_RATE, audio_bytes_to_np)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "Go36QW9AS_CD",
        "colab": {}
      },
      "source": [
        "#@title Upload Audio File\n",
        "#@markdown * Either use one of the provided examples or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "example_or_upload = \"Singing - Somwhere over the rainbow\"  #@param [\"Singing - Somwhere over the rainbow\", \"Singing - Easy\", \"Guitar - Hysteria\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "examples_path = \"/content/ddsp_demo/sources/audio_examples/\"\n",
        "\n",
        "if example_or_upload is not \"Upload (.mp3 or .wav)\":\n",
        "  def get_example(example):\n",
        "    return {\n",
        "        \"Singing - Somwhere over the rainbow\": 'singing-somewhere_over_the_rainbow.mp3',\n",
        "        \"Singing - Easy\": 'singing-commodores_easy.mp3',\n",
        "        \"Guitar - Hysteria\": 'guitar-muse_hysteria.mp3'\n",
        "    }[example]\n",
        "\n",
        "  example_path = examples_path + get_example(example_or_upload)\n",
        "\n",
        "  with open(example_path, \"rb\") as file_audio:\n",
        "    audio_as_bytes = file_audio.read()\n",
        "  audio = audio_bytes_to_np(audio_as_bytes)\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.eval_util.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz']))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "wmSGDWM5yyjm",
        "colab": {}
      },
      "source": [
        "#@title Choose a model\n",
        "\n",
        "model = 'Double Bass (34,800 steps)' #@param ['Double Bass (34,800 steps)', 'Double Bass (18,000 steps)', 'Acoustic Guitar (91,800 steps)', 'Electric Guitar (11,400 steps)']\n",
        "MODEL = model\n",
        "\n",
        "models_path = '/content/ddsp_demo/sources/models'\n",
        "\n",
        "def get_model_dir(x):\n",
        "    return {\n",
        "        'Double Bass (18,000 steps)': '/dbass_18000',\n",
        "        'Double Bass (34,800 steps)': '/dbass_34800',\n",
        "        'Acoustic Guitar (91,800 steps)': '/acoustic_91800',\n",
        "        'Electric Guitar (11,400 steps)': '/electric_11400'\n",
        "    }[x]\n",
        "\n",
        "model_dir = models_path + get_model_dir(model)\n",
        "gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Additive.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "gin_params = [\n",
        "    'Additive.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "uQFUlIJ_5r36",
        "colab": {}
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "#@markdown Manual adjustments:\n",
        "#@markdown * <b>f0_octave_shift</b> - Shift the fundmental frequency to a more natural register.\n",
        "#@markdown * <b>f0_confidence_threshold</b> - Silence audio below a threshold.\n",
        "#@markdown * <b>loudness_db_shift</b> - Adjsut the overall loudness level.\n",
        "f0_octave_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "f0_confidence_threshold =  0 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "loudness_db_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "#@markdown You might get more realistic sounds by shifting a few dB, or try going extreme and see what weird sounds you can make...\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, f0_octave_shift=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (f0_octave_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def mask_by_confidence(audio_features, confidence_level=0.1):\n",
        "  \"\"\"For the violin model, the masking causes fast dips in loudness. \n",
        "  This quick transient is interpreted by the model as the \"plunk\" sound.\n",
        "  \"\"\"\n",
        "  mask_idx = audio_features['f0_confidence'] < confidence_level\n",
        "  audio_features['f0_hz'][mask_idx] = 0.0\n",
        "  # audio_features['loudness_db'][mask_idx] = -ddsp.spectral_ops.LD_RANGE\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_db_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, f0_octave_shift)\n",
        "audio_features_mod = mask_by_confidence(audio_features_mod, f0_confidence_threshold)\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'])\n",
        "ax[0].plot(audio_features_mod['loudness_db'])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "ax[0].legend(['Original','Adjusted'])\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz']))\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "ax[1].legend(['Original','Adjusted'])\n",
        "\n",
        "ax[2].plot(audio_features_mod['f0_confidence'])\n",
        "ax[2].plot(np.ones_like(audio_features_mod['f0_confidence']) * f0_confidence_threshold)\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "SLwg1WkHCXQO",
        "colab": {}
      },
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_gen = model(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}